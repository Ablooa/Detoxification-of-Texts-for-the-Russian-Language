{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T21:30:22.497210Z",
     "iopub.status.busy": "2023-06-25T21:30:22.496823Z",
     "iopub.status.idle": "2023-06-25T21:30:36.109678Z",
     "shell.execute_reply": "2023-06-25T21:30:36.108521Z",
     "shell.execute_reply.started": "2023-06-25T21:30:22.497180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymystem3\n",
      "  Downloading pymystem3-0.2.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: requests in /Users/mariamitrankova/opt/anaconda3/lib/python3.9/site-packages (from pymystem3) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/mariamitrankova/opt/anaconda3/lib/python3.9/site-packages (from requests->pymystem3) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mariamitrankova/opt/anaconda3/lib/python3.9/site-packages (from requests->pymystem3) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mariamitrankova/opt/anaconda3/lib/python3.9/site-packages (from requests->pymystem3) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/mariamitrankova/opt/anaconda3/lib/python3.9/site-packages (from requests->pymystem3) (1.26.11)\n",
      "Installing collected packages: pymystem3\n",
      "Successfully installed pymystem3-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T22:54:00.213176Z",
     "iopub.status.busy": "2023-06-25T22:54:00.212115Z",
     "iopub.status.idle": "2023-06-25T22:54:00.220201Z",
     "shell.execute_reply": "2023-06-25T22:54:00.218646Z",
     "shell.execute_reply.started": "2023-06-25T22:54:00.213138Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymystem3 import Mystem\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T20:33:55.008385Z",
     "iopub.status.busy": "2023-06-25T20:33:55.008034Z",
     "iopub.status.idle": "2023-06-25T20:33:55.061349Z",
     "shell.execute_reply": "2023-06-25T20:33:55.060275Z",
     "shell.execute_reply.started": "2023-06-25T20:33:55.008357Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T23:35:47.295820Z",
     "iopub.status.busy": "2023-06-25T23:35:47.295148Z",
     "iopub.status.idle": "2023-06-25T23:35:47.397768Z",
     "shell.execute_reply": "2023-06-25T23:35:47.396896Z",
     "shell.execute_reply.started": "2023-06-25T23:35:47.295790Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/russian-language-toxic-comments/labeled.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6r/tphhr3f17_9gplh9_dcrhzk40000gn/T/ipykernel_92194/1763999205.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input/russian-language-toxic-comments/labeled.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/russian-language-toxic-comments/labeled.csv'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('/kaggle/input/russian-language-toxic-comments/labeled.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T23:35:49.286175Z",
     "iopub.status.busy": "2023-06-25T23:35:49.285715Z",
     "iopub.status.idle": "2023-06-25T23:35:50.147758Z",
     "shell.execute_reply": "2023-06-25T23:35:50.146624Z",
     "shell.execute_reply.started": "2023-06-25T23:35:49.286129Z"
    }
   },
   "outputs": [],
   "source": [
    "data['toxic'] = data['toxic'].astype(int)\n",
    "data = data.replace('\\n',' ', regex=True)\n",
    "data = data.replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
    "data['comment'] = data['comment'].str.lower()\n",
    "data = data.replace('[^а-яА-я]', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T23:35:51.871947Z",
     "iopub.status.busy": "2023-06-25T23:35:51.871578Z",
     "iopub.status.idle": "2023-06-25T23:35:51.882226Z",
     "shell.execute_reply": "2023-06-25T23:35:51.881163Z",
     "shell.execute_reply.started": "2023-06-25T23:35:51.871918Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>верблюдов то за что  дебилы  бл</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>хохлы  это отдушина затюканого россиянина  мол...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>собаке   собачья смерть</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>страницу обнови  дебил  это тоже не оскорблени...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>тебя не убедил   страничный пдф в том  что скр...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  toxic\n",
       "0                верблюдов то за что  дебилы  бл          1\n",
       "1  хохлы  это отдушина затюканого россиянина  мол...      1\n",
       "2                           собаке   собачья смерть       1\n",
       "3  страницу обнови  дебил  это тоже не оскорблени...      1\n",
       "4  тебя не убедил   страничный пдф в том  что скр...      1"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T22:54:39.788132Z",
     "iopub.status.busy": "2023-06-25T22:54:39.787748Z",
     "iopub.status.idle": "2023-06-25T22:55:34.119151Z",
     "shell.execute_reply": "2023-06-25T22:55:34.116321Z",
     "shell.execute_reply.started": "2023-06-25T22:54:39.788099Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14412 [00:00<?, ?it/s]/tmp/ipykernel_27/4109446096.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['comment'][i] = lemmat(data[\"comment\"][i])\n",
      "100%|██████████| 14412/14412 [00:54<00:00, 265.34it/s]\n"
     ]
    }
   ],
   "source": [
    "mystem = Mystem()\n",
    "\n",
    "def lemmat(comment):\n",
    "    word_list = word_tokenize(comment)\n",
    "    lemmas = [mystem.lemmatize(w)[0] for w in word_list if not w.lower() in stop_list]\n",
    "    return lemmas\n",
    "\n",
    "for i in tqdm(range(len(data))):\n",
    "    data['comment'][i] = lemmat(data[\"comment\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T22:55:34.122344Z",
     "iopub.status.busy": "2023-06-25T22:55:34.121613Z",
     "iopub.status.idle": "2023-06-25T22:55:34.176866Z",
     "shell.execute_reply": "2023-06-25T22:55:34.175684Z",
     "shell.execute_reply.started": "2023-06-25T22:55:34.122301Z"
    }
   },
   "outputs": [],
   "source": [
    "data['comment'] = data['comment'].apply(lambda x: ', '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T22:55:34.185435Z",
     "iopub.status.busy": "2023-06-25T22:55:34.182227Z",
     "iopub.status.idle": "2023-06-25T22:55:41.322417Z",
     "shell.execute_reply": "2023-06-25T22:55:41.321333Z",
     "shell.execute_reply.started": "2023-06-25T22:55:34.185387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "      <th>tokenized_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>верблюд, ?, дебил, ,\\n, бл, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[верблюд, ,, ?, ,, дебил, ,, ,, ,, бл, ,, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>хохол, ,\\n, это, отдушина, затюканый, россияни...</td>\n",
       "      <td>1</td>\n",
       "      <td>[хохол, ,, ,, ,, это, ,, отдушина, ,, затюканы...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>собака, -\\n, собачий, смерть</td>\n",
       "      <td>1</td>\n",
       "      <td>[собака, ,, -, ,, собачий, ,, смерть]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>страница, обновлять, ,\\n, дебил, ., это, оскор...</td>\n",
       "      <td>1</td>\n",
       "      <td>[страница, ,, обновлять, ,, ,, ,, дебил, ,, .,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>убеждать, 6, пдф, ,\\n, скрипалый, отравлять, р...</td>\n",
       "      <td>1</td>\n",
       "      <td>[убеждать, ,, 6, ,, пдф, ,, ,, ,, скрипалый, ,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  toxic  \\\n",
       "0                    верблюд, ?, дебил, ,\\n, бл, ...      1   \n",
       "1  хохол, ,\\n, это, отдушина, затюканый, россияни...      1   \n",
       "2                       собака, -\\n, собачий, смерть      1   \n",
       "3  страница, обновлять, ,\\n, дебил, ., это, оскор...      1   \n",
       "4  убеждать, 6, пдф, ,\\n, скрипалый, отравлять, р...      1   \n",
       "\n",
       "                                   tokenized_comment  \n",
       "0     [верблюд, ,, ?, ,, дебил, ,, ,, ,, бл, ,, ...]  \n",
       "1  [хохол, ,, ,, ,, это, ,, отдушина, ,, затюканы...  \n",
       "2              [собака, ,, -, ,, собачий, ,, смерть]  \n",
       "3  [страница, ,, обновлять, ,, ,, ,, дебил, ,, .,...  \n",
       "4  [убеждать, ,, 6, ,, пдф, ,, ,, ,, скрипалый, ,...  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "data['tokenized_comment'] = data['comment'].apply(nltk.word_tokenize)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T22:55:41.325844Z",
     "iopub.status.busy": "2023-06-25T22:55:41.324573Z",
     "iopub.status.idle": "2023-06-25T22:55:41.889523Z",
     "shell.execute_reply": "2023-06-25T22:55:41.888306Z",
     "shell.execute_reply.started": "2023-06-25T22:55:41.325804Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_comments = data['tokenized_comment'].apply(lambda x: ' '.join(x))\n",
    "tfidf_vectors = vectorizer.fit_transform(tokenized_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T22:55:51.673384Z",
     "iopub.status.busy": "2023-06-25T22:55:51.672947Z",
     "iopub.status.idle": "2023-06-25T22:55:51.689453Z",
     "shell.execute_reply": "2023-06-25T22:55:51.688465Z",
     "shell.execute_reply.started": "2023-06-25T22:55:51.673350Z"
    }
   },
   "outputs": [],
   "source": [
    "X = tfidf_vectors\n",
    "y = data['toxic']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T22:55:53.579191Z",
     "iopub.status.busy": "2023-06-25T22:55:53.578842Z",
     "iopub.status.idle": "2023-06-25T22:55:53.685686Z",
     "shell.execute_reply": "2023-06-25T22:55:53.684554Z",
     "shell.execute_reply.started": "2023-06-25T22:55:53.579162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc: 0.779036482404578\n",
      "test acc: 0.7861553202119407\n"
     ]
    }
   ],
   "source": [
    "N = 14412\n",
    "l2_coef = 0.5\n",
    "alpha = 2\n",
    "beta = 2\n",
    "batch_size = int(N/20)\n",
    "\n",
    "clf = LogisticRegression(C=1/(2*l2_coef), solver='sag')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_val = clf.predict(X_val)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "print(\"val acc:\", balanced_accuracy_score(y_val, y_pred_val))\n",
    "print(\"test acc:\", balanced_accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все слова с весами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T23:34:57.531094Z",
     "iopub.status.busy": "2023-06-25T23:34:57.530372Z",
     "iopub.status.idle": "2023-06-25T23:34:57.583360Z",
     "shell.execute_reply": "2023-06-25T23:34:57.581128Z",
     "shell.execute_reply.started": "2023-06-25T23:34:57.531061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word    weight\n",
      "0    верблюд  0.282320\n",
      "1      дебил  3.756881\n",
      "2         бл  0.611891\n",
      "3      хохол  5.117917\n",
      "4        это -1.296658\n",
      "5   отдушина  0.156241\n",
      "6  затюканый  0.000000\n",
      "7  россиянин  0.626020\n",
      "8        мол  0.428603\n",
      "9        вон  0.692230\n"
     ]
    }
   ],
   "source": [
    "weights = clf.coef_[0]\n",
    "word_weights = {word: weights[i] for word, i in vectorizer.vocabulary_.items()}\n",
    "word_weight_pairs = [(word, weight) for word, weight in word_weights.items()]\n",
    "df = pd.DataFrame(word_weight_pairs, columns=['word', 'weight'])\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Только плохие:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T23:35:06.561739Z",
     "iopub.status.busy": "2023-06-25T23:35:06.561383Z",
     "iopub.status.idle": "2023-06-25T23:35:06.571117Z",
     "shell.execute_reply": "2023-06-25T23:35:06.570138Z",
     "shell.execute_reply.started": "2023-06-25T23:35:06.561710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          word    weight\n",
      "1        дебил  3.756881\n",
      "3        хохол  5.117917\n",
      "10      хохлов  3.791348\n",
      "25      писать  1.479857\n",
      "27        твой  3.520533\n",
      "38      ватник  1.629258\n",
      "48       тупой  3.984838\n",
      "84       ебать  2.947675\n",
      "86       шизик  1.743394\n",
      "87  обсираться  1.511760\n"
     ]
    }
   ],
   "source": [
    "filtered_df = df[df['weight'] > 1]\n",
    "print(filtered_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
